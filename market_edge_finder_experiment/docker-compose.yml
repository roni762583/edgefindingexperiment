version: '3.8'

# Enable BuildKit for advanced caching
x-buildkit-config: &buildkit-config
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

services:
  # Data download service - Optimized for OANDA data acquisition
  data-downloader:
    build:
      context: .
      dockerfile: Dockerfile
      target: data-downloader
      cache_from:
        - edge-finder:deps
        - edge-finder:base
      args:
        - BUILDKIT_INLINE_CACHE=1
    container_name: edge-finder-downloader
    environment:
      - PYTHONPATH=/app
      - OANDA_ENVIRONMENT=${OANDA_ENVIRONMENT:-practice}
      - DATA_DOWNLOAD_MODE=true
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    env_file:
      - .env
    volumes:
      # Mount local directories for persistent data
      - ./data:/data
      - ./logs:/logs
      - ./results:/results
      # Cache directories for performance
      - pip-cache:/root/.cache/pip
      - torch-cache:/cache/torch
      - numba-cache:/cache/numba
    networks:
      - edge-finder-network
    restart: "no"  # One-time download task
    command: ["--download-data", "--instruments=all", "--granularity=H1", "--days=1095", "--progress"]
    healthcheck:
      test: ["CMD", "python", "-c", "import v20; import pandas; print('✅ Dependencies ready')"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s

  # Training service for TCNAE and LightGBM - Optimized with cache mounts
  training-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
      cache_from:
        - edge-finder:deps
        - edge-finder:base
      args:
        - BUILDKIT_INLINE_CACHE=1
        - USE_CUDA=${USE_CUDA:-false}
    container_name: edge-finder-training
    environment:
      - PYTHONPATH=/app
      - CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-}
      - OMP_NUM_THREADS=4
      - MKL_NUM_THREADS=4
      - TORCH_HOME=/cache/torch
      - NUMBA_CACHE_DIR=/cache/numba
    env_file:
      - .env
    volumes:
      # Persistent data volumes
      - ./data:/data:ro  # Read-only data access
      - ./models:/models
      - ./logs:/logs
      - ./results:/results
      # Performance cache volumes
      - torch-cache:/cache/torch
      - numba-cache:/cache/numba
      - pip-cache:/root/.cache/pip
    networks:
      - edge-finder-network
    restart: unless-stopped
    command: ["python", "scripts/run_training.py", "--config", "configs/production_config.yaml"]
    depends_on:
      data-downloader:
        condition: service_completed_successfully
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 6G
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; import lightgbm; import v20; print('✅ Training environment ready')"]
      interval: 60s
      timeout: 20s
      retries: 3
      start_period: 180s

  # Validation and evaluation service
  validation-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: edge-finder-validation
    environment:
      - PYTHONPATH=/app
    env_file:
      - .env
    volumes:
      - ./data:/app/data:ro
      - ./models:/app/models:ro
      - ./evaluation:/app/evaluation
      - ./logs:/app/logs
    networks:
      - edge-finder-network
    restart: unless-stopped
    command: ["python", "scripts/run_evaluation.py"]
    depends_on:
      - training-service
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  # Live inference service
  inference-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: edge-finder-inference
    environment:
      - PYTHONPATH=/app
      - OANDA_ENVIRONMENT=${OANDA_ENVIRONMENT:-live}
    env_file:
      - .env
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs
    networks:
      - edge-finder-network
    restart: unless-stopped
    command: ["python", "scripts/run_inference.py"]
    depends_on:
      - validation-service
    ports:
      - "8080:8080"  # Prediction API endpoint
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Development service (optional)
  dev-service:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    container_name: edge-finder-dev
    environment:
      - PYTHONPATH=/app
    env_file:
      - .env
    volumes:
      - .:/app
      - ./data:/app/data
      - ./models:/app/models
      - ./logs:/app/logs
    networks:
      - edge-finder-network
    ports:
      - "8888:8888"  # Jupyter notebook
      - "6006:6006"  # TensorBoard
    command: ["bash"]
    stdin_open: true
    tty: true
    profiles:
      - dev

  # Monitoring and logging (optional)
  monitoring-service:
    image: grafana/grafana:latest
    container_name: edge-finder-monitoring
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - edge-finder-network
    ports:
      - "3000:3000"
    restart: unless-stopped
    profiles:
      - monitoring

  # Database for model metadata and results
  database:
    image: postgres:15
    container_name: edge-finder-db
    environment:
      - POSTGRES_DB=${DB_NAME:-edge_finder}
      - POSTGRES_USER=${DB_USER:-postgres}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-postgres}
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    networks:
      - edge-finder-network
    ports:
      - "5432:5432"
    restart: unless-stopped
    profiles:
      - database

networks:
  edge-finder-network:
    driver: bridge
    name: edge-finder-network

volumes:
  # Persistent cache volumes for build optimization
  pip-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/pip  # Use host pip cache for persistence
  torch-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/torch  # Use host torch cache
  numba-cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${HOME}/.cache/numba  # Use host numba cache
  
  # Application data volumes
  grafana-storage:
    driver: local
  postgres-data:
    driver: local